{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoA Implementation\n",
    "\n",
    "## Team Members: Amitesh Shekhar(22B0014), Toshan Achintya Golla(22B2234), Vatsal Melwani (22B0396)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T00:14:39.669898Z",
     "iopub.status.busy": "2025-11-25T00:14:39.669699Z",
     "iopub.status.idle": "2025-11-25T00:14:39.686033Z",
     "shell.execute_reply": "2025-11-25T00:14:39.685254Z",
     "shell.execute_reply.started": "2025-11-25T00:14:39.669884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Base model to quantize\n",
    "MODEL_ID = \"facebook/opt-125m\"\n",
    "\n",
    "# Device setup: use GPU if available (Kaggle T4/P100), otherwise CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# BoA / GPTQ calibration and quantization hyperparameters\n",
    "SEQ_LEN = 2048      # sequence length for each calibration sample\n",
    "NSAMPLES = 128      # number of calibration samples\n",
    "Target_Bits = 2     # quantization precision (e.g., 3 -> INT3)\n",
    "Group_Size = 128    # group size for per-group quantization parameters\n",
    "\n",
    "# Safety check for GPU\n",
    "if DEVICE == \"cpu\":\n",
    "    print(\"⚠️ WARNING: You are running on CPU. For speed, enable GPU \")\n",
    "\n",
    "print(f\"Running on {DEVICE} with model {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T00:14:39.687340Z",
     "iopub.status.busy": "2025-11-25T00:14:39.687153Z",
     "iopub.status.idle": "2025-11-25T00:14:39.704388Z",
     "shell.execute_reply": "2025-11-25T00:14:39.703756Z",
     "shell.execute_reply.started": "2025-11-25T00:14:39.687325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_wikitext2(tokenizer, nsamples, seqlen):\n",
    "    \"\"\"\n",
    "    Load calibration and evaluation data from WikiText-2.\n",
    "\n",
    "    We:\n",
    "      - build a long tokenized training text\n",
    "      - slice nsamples windows of length seqlen uniformly at random\n",
    "      - return those windows as calibration data\n",
    "\n",
    "    Returns:\n",
    "      trainloader : list of (inp, tar) pairs, but only inp is used in our code\n",
    "      testdata    : Hugging Face Dataset for the test split\n",
    "    \"\"\"\n",
    "    traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    testdata  = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    \n",
    "    # Tokenize the entire training corpus as a single long stream\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        # sample a random start index for a window of length seqlen\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "\n",
    "        inp = trainenc.input_ids[:, i:j]  # shape: (1, seqlen)\n",
    "        tar = inp.clone()\n",
    "        # This masking is not used later, but kept for completeness / compatibility\n",
    "        tar[:, :-1] = -100                # ignore all but the last token\n",
    "        trainloader.append((inp, tar))\n",
    "\n",
    "    return trainloader, testdata\n",
    "\n",
    "\n",
    "def quantize_scalar(x, scale, zero, maxq):\n",
    "    \"\"\"\n",
    "    Elementwise uniform quantization + dequantization:\n",
    "        q = clamp(round(x / scale) + zero, 0, maxq)\n",
    "        x_hat = scale * (q - zero)\n",
    "\n",
    "    x, scale, zero should be broadcastable to the same shape.\n",
    "    \"\"\"\n",
    "    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n",
    "    return scale * (q - zero)\n",
    "\n",
    "\n",
    "def find_params(w, bits=Target_Bits, group_size=Group_Size):\n",
    "    \"\"\"\n",
    "    Compute simple per-group min-max quantization parameters.\n",
    "\n",
    "    Args:\n",
    "      w          : weight matrix (rows, cols)\n",
    "      bits       : number of bits (e.g. 3 for INT3)\n",
    "      group_size : number of weights per group along last dimension\n",
    "\n",
    "    Returns:\n",
    "      scale : tensor of shape (num_groups, 1)\n",
    "      zero  : tensor of shape (num_groups, 1)\n",
    "      maxq  : integer max quantized value (2^bits - 1)\n",
    "    \"\"\"\n",
    "    dev = w.device\n",
    "    maxq = 2**bits - 1\n",
    "\n",
    "    shape = w.shape\n",
    "    if group_size > 0:\n",
    "        # reshape into (num_groups, group_size)\n",
    "        w = w.reshape(-1, group_size)\n",
    "\n",
    "    # per-group min and max\n",
    "    wmax = torch.amax(w, dim=1, keepdim=True)\n",
    "    wmin = torch.amin(w, dim=1, keepdim=True)\n",
    "\n",
    "    # avoid zero dynamic range by enforcing at least 1e-5 in max\n",
    "    wmax = torch.max(wmax, torch.tensor(1e-5, device=w.device))\n",
    "\n",
    "    scale = (wmax - wmin) / maxq\n",
    "    zero = torch.round(-wmin / scale)\n",
    "\n",
    "    return scale.to(dev), zero.to(dev), maxq\n",
    "\n",
    "def get_wikitext2(tokenizer, nsamples, seqlen):\n",
    "    \"\"\"\n",
    "    Load calibration and evaluation data from WikiText-2.\n",
    "\n",
    "    We:\n",
    "      - build a long tokenized training text\n",
    "      - slice nsamples windows of length seqlen uniformly at random\n",
    "      - return those windows as calibration data\n",
    "\n",
    "    Returns:\n",
    "      trainloader : list of (inp, tar) pairs, but only inp is used in our code\n",
    "      testdata    : Hugging Face Dataset for the test split\n",
    "    \"\"\"\n",
    "    traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    testdata  = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    \n",
    "    # Tokenize the entire training corpus as a single long stream\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        # sample a random start index for a window of length seqlen\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "\n",
    "        inp = trainenc.input_ids[:, i:j]  # shape: (1, seqlen)\n",
    "        tar = inp.clone()\n",
    "        # This masking is not used later, but kept for completeness / compatibility\n",
    "        tar[:, :-1] = -100                # ignore all but the last token\n",
    "        trainloader.append((inp, tar))\n",
    "\n",
    "    return trainloader, testdata\n",
    "\n",
    "\n",
    "def quantize_scalar(x, scale, zero, maxq):\n",
    "    \"\"\"\n",
    "    Elementwise uniform quantization + dequantization:\n",
    "        q = clamp(round(x / scale) + zero, 0, maxq)\n",
    "        x_hat = scale * (q - zero)\n",
    "\n",
    "    x, scale, zero should be broadcastable to the same shape.\n",
    "    \"\"\"\n",
    "    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n",
    "    return scale * (q - zero)\n",
    "\n",
    "\n",
    "def find_params(w, bits=Target_Bits, group_size=Group_Size):\n",
    "    \"\"\"\n",
    "    Compute simple per-group min-max quantization parameters.\n",
    "\n",
    "    Args:\n",
    "      w          : weight matrix (rows, cols)\n",
    "      bits       : number of bits (e.g. 3 for INT3)\n",
    "      group_size : number of weights per group along last dimension\n",
    "\n",
    "    Returns:\n",
    "      scale : tensor of shape (num_groups, 1)\n",
    "      zero  : tensor of shape (num_groups, 1)\n",
    "      maxq  : integer max quantized value (2^bits - 1)\n",
    "    \"\"\"\n",
    "    dev = w.device\n",
    "    maxq = 2**bits - 1\n",
    "\n",
    "    shape = w.shape\n",
    "    if group_size > 0:\n",
    "        # reshape into (num_groups, group_size)\n",
    "        w = w.reshape(-1, group_size)\n",
    "\n",
    "    # per-group min and max\n",
    "    wmax = torch.amax(w, dim=1, keepdim=True)\n",
    "    wmin = torch.amin(w, dim=1, keepdim=True)\n",
    "\n",
    "    # avoid zero dynamic range by enforcing at least 1e-5 in max\n",
    "    wmax = torch.max(wmax, torch.tensor(1e-5, device=w.device))\n",
    "\n",
    "    scale = (wmax - wmin) / maxq\n",
    "    zero = torch.round(-wmin / scale)\n",
    "\n",
    "    return scale.to(dev), zero.to(dev), maxq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoA Quantizer Class (Relaxed BoA: Q/K use attention-aware Hessians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T00:14:39.724047Z",
     "iopub.status.busy": "2025-11-25T00:14:39.723857Z",
     "iopub.status.idle": "2025-11-25T00:14:39.744248Z",
     "shell.execute_reply": "2025-11-25T00:14:39.743429Z",
     "shell.execute_reply.started": "2025-11-25T00:14:39.724030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BoAQuantizer:\n",
    "    def __init__(self, layer, bits=None, group_size=None):\n",
    "        \"\"\"\n",
    "        layer      : the nn.Linear layer to be quantized\n",
    "        bits       : bit-width for this layer (if None, fall back to global Target_Bits)\n",
    "        group_size : quantization group size (if None, fall back to global Group_Size)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Wrapper around a linear layer to perform:\n",
    "          - GPTQ-style column quantization for all layers\n",
    "          - plus BoA-style row coupling for attention Q/K projections\n",
    "        \"\"\"\n",
    "        \n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "\n",
    "        self.rows = self.layer.weight.data.shape[0]\n",
    "        self.cols = self.layer.weight.data.shape[1]\n",
    "\n",
    "        # Store quantization configuration per instance\n",
    "        self.bits = bits if bits is not None else Target_Bits\n",
    "        self.group_size = group_size if group_size is not None else Group_Size\n",
    "\n",
    "        # Column Hessian for GPTQ: H_col = X^T X\n",
    "        self.H_col = torch.zeros((self.cols, self.cols), device=self.dev, dtype=torch.float32)\n",
    "\n",
    "        # Row Hessian for BoA (per head): built only for Q/K using other_input\n",
    "        self.H_row = None\n",
    "        self.nsamples = 0\n",
    "        self.is_boa_layer = False  # True only for Q/K, where other_input is provided\n",
    "\n",
    "    def add_batch(self, inp, other_input=None):\n",
    "        \"\"\"\n",
    "        Accumulate Hessian statistics from a batch of hidden states.\n",
    "\n",
    "        inp:          input activations to the linear layer (X)\n",
    "        other_input:  additional activations used for attention-aware Hessians\n",
    "                      (e.g., K when quantizing Q, and Q when quantizing K)\n",
    "        \"\"\"\n",
    "        # Always work in float32 for Hessian accumulation\n",
    "        if inp.dtype != torch.float32:\n",
    "            inp = inp.float()\n",
    "        if len(inp.shape) == 3:\n",
    "            # (batch, seq, hidden) -> (batch*seq, hidden)\n",
    "            inp = inp.reshape((-1, inp.shape[-1]))\n",
    "\n",
    "        # GPTQ column Hessian H_col += X^T X\n",
    "        self.H_col += torch.matmul(inp.t(), inp)\n",
    "\n",
    "        # If we have other_input, this layer is treated as BoA-aware\n",
    "        if other_input is not None:\n",
    "            self.is_boa_layer = True\n",
    "\n",
    "            if other_input.dtype != torch.float32:\n",
    "                other_input = other_input.float()\n",
    "            if len(other_input.shape) == 3:\n",
    "                other_input = other_input.reshape((-1, other_input.shape[-1]))\n",
    "\n",
    "            # Hard-coded for OPT-125M: 12 heads\n",
    "            num_heads = 12\n",
    "            head_dim = self.rows // num_heads\n",
    "\n",
    "            # Reshape to (tokens, num_heads, head_dim)\n",
    "            other_reshaped = other_input.view(-1, num_heads, head_dim)\n",
    "\n",
    "            # Initialize row Hessians per head if needed\n",
    "            if self.H_row is None:\n",
    "                self.H_row = [\n",
    "                    torch.zeros((head_dim, head_dim), device=self.dev, dtype=torch.float32)\n",
    "                    for _ in range(num_heads)\n",
    "                ]\n",
    "\n",
    "            # For each head, accumulate H_row += A^T A style term\n",
    "            for h in range(num_heads):\n",
    "                oh = other_reshaped[:, h, :]     # (tokens, head_dim)\n",
    "                self.H_row[h] += torch.matmul(oh.t(), oh)\n",
    "\n",
    "        self.nsamples += 1\n",
    "\n",
    "    def gptq_quantize_block(self, W_f, H_inv, bits=None):\n",
    "        \"\"\"\n",
    "        Quantize a weight block W_f using GPTQ-style algorithm:\n",
    "\n",
    "        - use simple per-group min-max quantization (find_params)\n",
    "        - apply column-wise decorrelation update using H_inv\n",
    "        \"\"\"\n",
    "\n",
    "        if bits is None:\n",
    "            bits = self.bits\n",
    "\n",
    "        \n",
    "        Q_f = torch.zeros_like(W_f)\n",
    "        E_f = torch.zeros_like(W_f)\n",
    "\n",
    "        rows, cols = W_f.shape\n",
    "\n",
    "        # Compute per-group scale and zero for all rows combined\n",
    "        scale, zero, maxq = find_params(W_f, bits=bits, group_size=self.group_size)\n",
    "\n",
    "        # Reshape scale/zero to (rows, num_groups)\n",
    "        groups_per_row = cols // self.group_size\n",
    "        scale = scale.reshape(rows, groups_per_row)\n",
    "        zero  = zero.reshape(rows, groups_per_row)\n",
    "\n",
    "        inv_diag = 1.0 / torch.diag(H_inv)\n",
    "\n",
    "        for i in range(cols):\n",
    "            w = W_f[:, i]        # i-th column of W_f, shape (rows,)\n",
    "            d = inv_diag[i]\n",
    "\n",
    "            # Select the group that this column belongs to\n",
    "            g_idx = i // self.group_size\n",
    "            s = scale[:, g_idx]  # per-row scale for this group\n",
    "            z = zero[:, g_idx]   # per-row zero for this group\n",
    "\n",
    "            # Uniform quantization to nearest grid point\n",
    "            q = quantize_scalar(w, s, z, maxq)\n",
    "            Q_f[:, i] = q\n",
    "\n",
    "            # GPTQ error term, scaled using H_inv diagonal\n",
    "            err = (w - q) / d\n",
    "            E_f[:, i] = err\n",
    "\n",
    "            # Decorrelate future columns using H_inv\n",
    "            if i < cols - 1:\n",
    "                update = torch.matmul(err.unsqueeze(1), H_inv[i, i+1:].unsqueeze(0))\n",
    "                W_f[:, i+1:] -= update\n",
    "\n",
    "        return Q_f, E_f\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Final quantization entry point.\n",
    "\n",
    "        Steps:\n",
    "          1. Build (damped) GPTQ Hessian H_col and invert it.\n",
    "          2. If layer is not BoA-aware: run GPTQ quantization only.\n",
    "          3. If BoA-aware (Q/K): compute row Hessians per head and apply\n",
    "             head-wise BoA row updates on top of GPTQ quantized weights.\n",
    "        \"\"\"\n",
    "        # Copy weights as float32 for numerical stability\n",
    "        W = self.layer.weight.data.float()\n",
    "        rows, cols = W.shape\n",
    "\n",
    "        # GPTQ column Hessian with Tikhonov damping\n",
    "        H_col = self.H_col\n",
    "        damp = 0.01 * torch.mean(torch.diag(H_col))\n",
    "        diag = torch.arange(cols, device=self.dev)\n",
    "        H_col[diag, diag] += damp\n",
    "\n",
    "        try:\n",
    "            L = torch.linalg.cholesky(H_col)\n",
    "            L_inv = torch.linalg.inv(L)\n",
    "            H_inv = torch.matmul(L_inv.t(), L_inv)\n",
    "        except Exception:\n",
    "            print(\"Hessian singular! Using Identity for H_inv.\")\n",
    "            H_inv = torch.eye(cols, device=self.dev)\n",
    "\n",
    "        # If no BoA row Hessian, just do GPTQ\n",
    "        if not self.is_boa_layer or self.H_row is None:\n",
    "            Q, _ = self.gptq_quantize_block(W, H_inv)\n",
    "            self.layer.weight.data = Q.to(self.layer.weight.dtype)\n",
    "            return\n",
    "\n",
    "        # Otherwise, use relaxed BoA for Q/K\n",
    "        num_heads = 12\n",
    "        head_dim = rows // num_heads\n",
    "        U_rows = []\n",
    "\n",
    "        # Build U_row per head from H_row (damped inverse)\n",
    "        for h in range(num_heads):\n",
    "            Hr = self.H_row[h]\n",
    "            damp_r = 0.01 * torch.mean(torch.diag(Hr))\n",
    "            Hr[torch.arange(head_dim), torch.arange(head_dim)] += damp_r\n",
    "\n",
    "            try:\n",
    "                Lr = torch.linalg.cholesky(Hr)\n",
    "                Lr_inv = torch.linalg.inv(Lr)\n",
    "                Hr_inv = torch.matmul(Lr_inv.t(), Lr_inv)\n",
    "                Ur = torch.linalg.cholesky(Hr_inv).t()\n",
    "                U_rows.append(Ur)\n",
    "            except Exception:\n",
    "                U_rows.append(torch.eye(head_dim, device=self.dev))\n",
    "\n",
    "        Q_final = torch.zeros_like(W)\n",
    "\n",
    "        # Process each \"row index within a head\"\n",
    "        for j in tqdm(range(head_dim), leave=False):\n",
    "            # These are the actual row indices in W belonging to row j of each head\n",
    "            indices = [j + h * head_dim for h in range(num_heads)]\n",
    "            W_j = W[indices, :].clone()\n",
    "\n",
    "            # GPTQ quantization for stacked rows from all heads\n",
    "            Q_j, E_j = self.gptq_quantize_block(W_j, H_inv)\n",
    "            Q_final[indices, :] = Q_j\n",
    "\n",
    "            # BoA row update within each head, using U_rows\n",
    "            for h in range(num_heads):\n",
    "                Ur = U_rows[h]\n",
    "                diag_val = Ur[j, j]\n",
    "\n",
    "                if j < head_dim - 1:\n",
    "                    v = Ur[j+1:, j].unsqueeze(1)  # vector for rows below j\n",
    "                    err_vec = E_j[h].unsqueeze(0)\n",
    "                    update_factor = (v / diag_val)\n",
    "                    target_indices = [(k + h * head_dim) for k in range(j+1, head_dim)]\n",
    "                    W[target_indices, :] -= torch.matmul(update_factor, err_vec)\n",
    "\n",
    "        # Write back quantized weights to the original layer, preserving dtype\n",
    "        self.layer.weight.data = Q_final.to(self.layer.weight.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Quantization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T00:14:39.745098Z",
     "iopub.status.busy": "2025-11-25T00:14:39.744895Z",
     "iopub.status.idle": "2025-11-25T00:14:39.760610Z",
     "shell.execute_reply": "2025-11-25T00:14:39.759876Z",
     "shell.execute_reply.started": "2025-11-25T00:14:39.745083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_quantization(bits=None):\n",
    "\n",
    "    if bits is None:\n",
    "        bits = Target_Bits\n",
    "\n",
    "    \"\"\"\n",
    "    Run relaxed BoA quantization over all Transformer layers.\n",
    "\n",
    "    Strategy:\n",
    "      - Load model in fp16.\n",
    "      - Capture inputs to the first layer (decoder layer 0).\n",
    "      - For each decoder layer:\n",
    "          * move layer to GPU\n",
    "          * collect Hessian statistics via forward passes\n",
    "          * run BoAQuantizer.execute() for each linear module\n",
    "          * propagate outputs to use as inputs for next layer\n",
    "          * move layer back to CPU\n",
    "      - Return quantized model on DEVICE.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n[run_quantization] Quantizing with bit-width = {bits}\")\n",
    "    \n",
    "    # Clean memory before starting\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()  # always eval mode during quantization\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    \n",
    "    print(\"Loading calibration data...\")\n",
    "    trainloader, _ = get_wikitext2(tokenizer, NSAMPLES, SEQ_LEN)\n",
    "    \n",
    "    # Hook to capture inputs into the first decoder layer\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            if isinstance(inp, tuple):\n",
    "                inp = inp[0]\n",
    "            # store activations as fp16 on CPU for memory efficiency\n",
    "            inps.append(inp.detach().to(\"cpu\", dtype=torch.float16))\n",
    "            raise ValueError  # stop the forward after capturing\n",
    "\n",
    "    layers = model.model.decoder.layers\n",
    "    \n",
    "    # Keep embeddings on GPU\n",
    "    model.model.decoder.embed_tokens    = model.model.decoder.embed_tokens.to(DEVICE)\n",
    "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(DEVICE)\n",
    "    \n",
    "    inps = []\n",
    "    # Replace first layer with catcher, temporarily\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    \n",
    "    print(\"Capturing inputs...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(trainloader, desc=\"Capturing\"):\n",
    "            try:\n",
    "                model(batch[0].to(DEVICE))\n",
    "            except ValueError:\n",
    "                # Raised intentionally by Catcher to break after first layer\n",
    "                pass\n",
    "\n",
    "    # Restore original first layer\n",
    "    layers[0] = layers[0].module  \n",
    "    \n",
    "    # Stack captured activations: shape (total_tokens, hidden_size)\n",
    "    inps = torch.cat(inps, dim=0).view(-1, inps[0].shape[-1])\n",
    "    print(f\"Captured Inputs Shape (tokens x hidden): {inps.shape}\")\n",
    "\n",
    "    print(\"Starting Quantization...\")\n",
    "    for i in tqdm(range(len(layers)), desc=\"Quantizing Layers\"):\n",
    "        layer = layers[i]\n",
    "        # Move current layer to GPU\n",
    "        layer = layer.to(DEVICE)\n",
    "        \n",
    "        # Submodules to quantize in each transformer block\n",
    "        subset = {\n",
    "            \"self_attn.q_proj\":  layer.self_attn.q_proj,\n",
    "            \"self_attn.k_proj\":  layer.self_attn.k_proj,\n",
    "            \"self_attn.v_proj\":  layer.self_attn.v_proj,\n",
    "            \"self_attn.out_proj\": layer.self_attn.out_proj,\n",
    "            \"fc1\":               layer.fc1,\n",
    "            \"fc2\":               layer.fc2,\n",
    "        }\n",
    "        \n",
    "        # Wrap each linear submodule with BoAQuantizer\n",
    "                # CHANGED: pass bits and group_size into each BoAQuantizer\n",
    "        quantizers = {\n",
    "            name: BoAQuantizer(m, bits=bits, group_size=Group_Size)\n",
    "            for name, m in subset.items()\n",
    "        }\n",
    "        \n",
    "        # Hook to collect Hessian stats for each submodule\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                # inp[0] is the hidden state for that module\n",
    "                quantizers[name].add_batch(inp[0].data)\n",
    "            return tmp\n",
    "\n",
    "        handles = [m.register_forward_hook(add_batch(n)) for n, m in subset.items()]\n",
    "        \n",
    "        # Compute how many batches we can form from captured activations\n",
    "        num_batches = inps.shape[0] // SEQ_LEN\n",
    "        \n",
    "        # First forward pass: collect GPTQ H_col for all modules,\n",
    "        # and BoA H_row for Q/K using X_norm, Q, K.\n",
    "        with torch.no_grad():\n",
    "            for j in range(num_batches):\n",
    "                batch_inp = inps[j*SEQ_LEN:(j+1)*SEQ_LEN].unsqueeze(0).to(DEVICE)  # (1, seqlen, hidden)\n",
    "                layer(batch_inp)\n",
    "\n",
    "                # LayerNorm output used to compute Q/K BoA Hessians\n",
    "                X_norm = layer.self_attn_layer_norm(batch_inp)\n",
    "                Q_mat  = layer.self_attn.q_proj(X_norm)\n",
    "                K_mat  = layer.self_attn.k_proj(X_norm)\n",
    "                \n",
    "                quantizers[\"self_attn.q_proj\"].add_batch(X_norm, other_input=K_mat)\n",
    "                quantizers[\"self_attn.k_proj\"].add_batch(X_norm, other_input=Q_mat)\n",
    "                \n",
    "                # Free intermediates\n",
    "                del X_norm, Q_mat, K_mat, batch_inp\n",
    "        \n",
    "        # Remove hooks after stats collection\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        \n",
    "        # Execute quantization for each module (internally uses BoA/GPTQ)\n",
    "        for name, quantizer in quantizers.items():\n",
    "            quantizer.execute()\n",
    "            \n",
    "        # Free quantizers and Hessians\n",
    "        del quantizers\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Second forward pass: propagate quantized outputs as new inputs\n",
    "        outs = []\n",
    "        with torch.no_grad():\n",
    "            for j in range(num_batches):\n",
    "                batch_inp = inps[j*SEQ_LEN:(j+1)*SEQ_LEN].unsqueeze(0).to(DEVICE)\n",
    "                out = layer(batch_inp)[0]  # (1, seqlen, hidden)\n",
    "                outs.append(out.cpu())\n",
    "                del batch_inp, out\n",
    "        \n",
    "        # Replace inps with concatenated outputs for next layer\n",
    "        del inps\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if len(outs) > 0:\n",
    "            # combine along the sequence dimension\n",
    "            inps = torch.cat(outs, dim=1).squeeze(0)  # (seqlen, hidden)\n",
    "            # flatten back to (tokens, hidden)\n",
    "            inps = inps.reshape(-1, inps.shape[-1])\n",
    "        \n",
    "        # Move quantized layer back to CPU to save VRAM\n",
    "        layers[i] = layer.cpu()\n",
    "        del layer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Once all layers are quantized, move entire model back to DEVICE\n",
    "    model.to(DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Utilities (Perplexity, Saving, and Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T00:14:39.761776Z",
     "iopub.status.busy": "2025-11-25T00:14:39.761462Z",
     "iopub.status.idle": "2025-11-25T00:14:39.786244Z",
     "shell.execute_reply": "2025-11-25T00:14:39.785690Z",
     "shell.execute_reply.started": "2025-11-25T00:14:39.761754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate WikiText PPL\n",
    "def compute_ppl(model, tokenizer, desc=\"PPL Eval\"):\n",
    "    \"\"\"\n",
    "    Computes perplexity (PPL) on WikiText-2 using sliding-window evaluation.\n",
    "\n",
    "    The model is run over the test split in chunks of length `stride`,\n",
    "    and loss is computed by shifting labels while masking unused tokens.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {desc} on WikiText-2 ---\")\n",
    "\n",
    "    # Load WikiText-2 test split\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "\n",
    "    # Tokenize entire dataset at once (stored on CPU)\n",
    "    enc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "\n",
    "    seq_len = enc.input_ids.size(1)\n",
    "    stride = 512\n",
    "    max_len = model.config.max_position_embeddings  # usually 2048 for OPT\n",
    "    nlls = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Slide model forward in windows of length `stride`\n",
    "        for i in tqdm(range(0, seq_len, stride), desc=\"PPL Progress\"):\n",
    "            j = min(i + max_len, seq_len)\n",
    "\n",
    "            # Input tokens for this window\n",
    "            inp = enc.input_ids[:, i:j].to(DEVICE)\n",
    "\n",
    "            # Labels: mask out all tokens except last (causal LM style)\n",
    "            tgt = inp.clone()\n",
    "            tgt[:, :- (j - i)] = -100  # ignore all but final shifted tokens\n",
    "\n",
    "            # Compute loss and accumulate it\n",
    "            loss = model(inp, labels=tgt).loss\n",
    "            nlls.append(loss)\n",
    "\n",
    "    # Convert negative log-likelihood to perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    print(\"PPL =\", float(ppl))\n",
    "    return ppl\n",
    "\n",
    "\n",
    "# Evaluate ARC-QA (Easy / Challenge)\n",
    "def evaluate_arc(model, tokenizer, subset=\"ARC-Easy\", max_samples=200):\n",
    "    \"\"\"\n",
    "    Zero-shot ARC evaluation using per-choice log-probabilities.\n",
    "\n",
    "    Each question has multiple choices; we evaluate the loss for each\n",
    "    (question + choice) pair and choose the answer with the highest score.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- ARC Eval: {subset} ---\")\n",
    "\n",
    "    # Load the chosen ARC subset (Easy or Challenge)\n",
    "    ds = load_dataset(\"ai2_arc\", subset, split=\"test\")\n",
    "\n",
    "    # Limit dataset for faster eval\n",
    "    ds = ds.select(range(min(max_samples, len(ds))))\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(ds, desc=\"ARC\"):\n",
    "            q = item[\"question\"]\n",
    "            choices = item[\"choices\"][\"text\"]\n",
    "            labels  = item[\"choices\"][\"label\"]\n",
    "            ans = item[\"answerKey\"]\n",
    "\n",
    "            # Compute score per choice = negative loss of (question + choice)\n",
    "            scores = []\n",
    "            for choice in choices:\n",
    "                enc = tokenizer(q + \" \" + choice, return_tensors=\"pt\").to(DEVICE)\n",
    "                loss = model(**enc, labels=enc.input_ids).loss.item()\n",
    "                scores.append(-loss)  # higher = better\n",
    "\n",
    "            # Pick the highest-scoring choice\n",
    "            pred = labels[np.argmax(scores)]\n",
    "            if pred == ans:\n",
    "                correct += 1\n",
    "\n",
    "    acc = correct / len(ds)\n",
    "    print(\"Accuracy =\", acc)\n",
    "    return acc, correct, len(ds)\n",
    "\n",
    "# Evaluate SQuAD-v1.1 Exact-Match & F1\n",
    "def evaluate_squad(model, tokenizer, max_samples=200):\n",
    "    \"\"\"\n",
    "    Runs a lightweight question-answering pipeline over SQuAD v1.1\n",
    "    and computes F1 using HuggingFace 'evaluate'.\n",
    "\n",
    "    Uses model.generate inside the QA pipeline to extract answer spans.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- SQuAD Evaluation (F1) ---\")\n",
    "\n",
    "    squad = load_dataset(\"squad\", split=\"validation\")\n",
    "    squad = squad.select(range(min(max_samples, len(squad))))\n",
    "\n",
    "    # Metric from HuggingFace evaluate package\n",
    "    metric = evaluate.load(\"squad\")\n",
    "\n",
    "    # Create QA pipeline\n",
    "    # (Do NOT pass device when model loaded via accelerate)\n",
    "    qa = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        top_k=1,\n",
    "        max_answer_len=50,\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for item in tqdm(squad, desc=\"SQuAD\"):\n",
    "        question = item[\"question\"]\n",
    "        context = item[\"context\"]\n",
    "        gold_text = item[\"answers\"][\"text\"][0]\n",
    "        gold_start = item[\"answers\"][\"answer_start\"][0]\n",
    "\n",
    "        # Try to extract predicted answer\n",
    "        try:\n",
    "            out = qa(question=question, context=context)\n",
    "            pred_text = out[\"answer\"]\n",
    "        except Exception:\n",
    "            pred_text = \"\"  # fallback for pipeline runtime errors\n",
    "\n",
    "        predictions.append({\"id\": item[\"id\"], \"prediction_text\": pred_text})\n",
    "        references.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"answers\": {\"text\": [gold_text], \"answer_start\": [gold_start]},\n",
    "        })\n",
    "\n",
    "    # Compute F1\n",
    "    scores = metric.compute(predictions=predictions, references=references)\n",
    "    print(\"SQuAD-F1 =\", scores[\"f1\"])\n",
    "    return scores[\"f1\"], len(squad)\n",
    "\n",
    "\n",
    "# CNN/DailyMail Summarization (BLEU + ROUGE-L)\n",
    "def evaluate_cnn_dm(model, tokenizer, max_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluates summarization quality on CNN/DailyMail using HF pipeline.\n",
    "\n",
    "    BLEU is computed using tokenized hypotheses and references.\n",
    "    ROUGE-L is computed using raw strings.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- CNN/DailyMail Summarization ---\")\n",
    "\n",
    "    ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation\")\n",
    "    ds = ds.select(range(min(max_samples, len(ds))))\n",
    "\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    # Summarization pipeline\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    preds_str = []        # raw model summaries\n",
    "    refs_str = []         # raw gold summaries\n",
    "    bleu_hypotheses = []  # tokenized predictions\n",
    "    bleu_references = []  # tokenized references\n",
    "\n",
    "    for item in tqdm(ds, desc=\"Summarization\"):\n",
    "        # Limit article length to avoid excessive generation time\n",
    "        article = item[\"article\"][:1500]\n",
    "        reference = item[\"highlights\"]\n",
    "\n",
    "        try:\n",
    "            out = summarizer(\n",
    "                article,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "                truncation=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            pred = out[0][\"summary_text\"]\n",
    "        except:\n",
    "            pred = \"\"\n",
    "\n",
    "        # Store for ROUGE\n",
    "        preds_str.append(pred)\n",
    "        refs_str.append(reference)\n",
    "\n",
    "        # Tokenize for BLEU\n",
    "        bleu_hypotheses.append(pred.strip().split())\n",
    "        bleu_references.append([reference.strip().split()])\n",
    "\n",
    "    # Convert lists to BLEU-compatible string format\n",
    "    bleu_predictions = [\" \".join(p) for p in bleu_hypotheses]\n",
    "    bleu_refs = [[ \" \".join(r) ] for ref in bleu_references for r in ref]\n",
    "\n",
    "    # Compute BLEU & ROUGE-L\n",
    "    bleu_score = bleu.compute(\n",
    "        predictions=bleu_predictions,\n",
    "        references=bleu_refs\n",
    "    )[\"bleu\"]\n",
    "\n",
    "    rouge_score = rouge.compute(\n",
    "        predictions=preds_str,\n",
    "        references=refs_str\n",
    "    )[\"rougeL\"]\n",
    "\n",
    "    print(\"BLEU:\", bleu_score)\n",
    "    print(\"ROUGE-L:\", rouge_score)\n",
    "    return bleu_score, rouge_score, len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Checking the Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the quantized model\n",
    "def save_quantized_model(model, tokenizer, out_dir=\"quantized_model\"):\n",
    "    \"\"\"\n",
    "    Save the quantized model and tokenizer in Hugging Face format.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f\"Saving quantized model to: {out_dir}\")\n",
    "    model.save_pretrained(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "# Sample Text Generation\n",
    "def test_generation(model, tokenizer, prompt=\"What would humans do if aliens attacked the earth?\"):\n",
    "    \"\"\"\n",
    "    Simple text generation demo to qualitatively inspect the quantized model.\n",
    "    \"\"\"\n",
    "    print(\"\\nGeneration demo (quantized model):\")\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,                      \n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=model.config.eos_token_id \n",
    "        )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-Precision vs Quantized Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T00:14:39.787078Z",
     "iopub.status.busy": "2025-11-25T00:14:39.786860Z",
     "iopub.status.idle": "2025-11-25T00:25:14.218820Z",
     "shell.execute_reply": "2025-11-25T00:25:14.218152Z",
     "shell.execute_reply.started": "2025-11-25T00:14:39.787062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) Load FP16 baseline model and tokenizer\n",
    "print(\"\\n=== Baseline: Loading FP16 Full-Precision Model ===\")\n",
    "fp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "fp_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# 2) Evaluate FP16 on WikiText-2 PPL\n",
    "fp_ppl = compute_ppl(fp_model, fp_tokenizer, desc=\"FP16 Baseline\")\n",
    "\n",
    "# 3) Evaluate FP16 on ARC-Easy and ARC-Challenge\n",
    "fp_arc_easy_acc, fp_arc_easy_correct, fp_arc_easy_total = evaluate_arc(\n",
    "    fp_model, fp_tokenizer, subset=\"ARC-Easy\", max_samples=200\n",
    ")\n",
    "fp_arc_ch_acc, fp_arc_ch_correct, fp_arc_ch_total = evaluate_arc(\n",
    "    fp_model, fp_tokenizer, subset=\"ARC-Challenge\", max_samples=200\n",
    ")\n",
    "\n",
    "# 4) Evaluate FP16 on SQuAD (F1)\n",
    "fp_squad_f1, fp_squad_n = evaluate_squad(\n",
    "    fp_model, fp_tokenizer, max_samples=200\n",
    ")\n",
    "\n",
    "# 5) Evaluate FP16 on CNN/DailyMail (BLEU + ROUGE-L)\n",
    "fp_bleu, fp_rougeL, fp_cnn_n = evaluate_cnn_dm(\n",
    "    fp_model, fp_tokenizer, max_samples=50\n",
    ")\n",
    "\n",
    "# 6) Compute theoretical FP16 model size (in MB)\n",
    "total_params = sum(p.numel() for p in fp_model.parameters())\n",
    "fp16_size_mb = total_params * 16 / 8 / 1e6  # 16 bits => 2 bytes\n",
    "\n",
    "print(f\"\\nFP16 Model Parameters: {total_params/1e6:.2f} M\")\n",
    "print(f\"Theoretical FP16 Model Size: {fp16_size_mb:.2f} MB\")\n",
    "\n",
    "# 7) Free FP model before heavy quantization to save VRAM\n",
    "del fp_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 8) Run quantization for current Target_Bits\n",
    "print(f\"\\n=== Running BoA Quantization (INT{Target_Bits}) ===\")\n",
    "q_model = run_quantization(bits=Target_Bits)\n",
    "q_tokenizer = fp_tokenizer  # same tokenizer\n",
    "\n",
    "# 9) Evaluate quantized model on WikiText-2 PPL\n",
    "q_ppl = compute_ppl(q_model, q_tokenizer, desc=f\"INT{Target_Bits} Quantized\")\n",
    "\n",
    "# 10) Evaluate quantized model on ARC-Easy and ARC-Challenge\n",
    "q_arc_easy_acc, q_arc_easy_correct, q_arc_easy_total = evaluate_arc(\n",
    "    q_model, q_tokenizer, subset=\"ARC-Easy\", max_samples=200\n",
    ")\n",
    "q_arc_ch_acc, q_arc_ch_correct, q_arc_ch_total = evaluate_arc(\n",
    "    q_model, q_tokenizer, subset=\"ARC-Challenge\", max_samples=200\n",
    ")\n",
    "\n",
    "# 11) Evaluate quantized model on SQuAD (F1)\n",
    "q_squad_f1, q_squad_n = evaluate_squad(\n",
    "    q_model, q_tokenizer, max_samples=200\n",
    ")\n",
    "\n",
    "# 12) Evaluate quantized model on CNN/DailyMail (BLEU + ROUGE-L)\n",
    "q_bleu, q_rougeL, q_cnn_n = evaluate_cnn_dm(\n",
    "    q_model, q_tokenizer, max_samples=50\n",
    ")\n",
    "\n",
    "# 13) Theoretical quantized size assuming pure INT{Target_Bits} storage\n",
    "quant_bits = Target_Bits\n",
    "quant_size_mb = total_params * quant_bits / 8 / 1e6\n",
    "compression_ratio = fp16_size_mb / quant_size_mb\n",
    "\n",
    "print(\"\\n=== Size and Compression Summary ===\")\n",
    "print(f\"INT{Target_Bits} Theoretical Size: {quant_size_mb:.2f} MB\")\n",
    "print(f\"Theoretical Compression Ratio (FP16 -> INT{Target_Bits}): {compression_ratio:.2f}x\")\n",
    "\n",
    "# 14) Save quantized model\n",
    "out_dir = f\"boa_opt125m_int{Target_Bits}\"\n",
    "save_quantized_model(q_model, q_tokenizer, out_dir=out_dir)\n",
    "\n",
    "# 15) Quick sanity check: generate some text from quantized model\n",
    "test_generation(q_model, q_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final summary of all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T00:25:14.219922Z",
     "iopub.status.busy": "2025-11-25T00:25:14.219664Z",
     "iopub.status.idle": "2025-11-25T00:25:14.226755Z",
     "shell.execute_reply": "2025-11-25T00:25:14.226028Z",
     "shell.execute_reply.started": "2025-11-25T00:25:14.219895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Metrics Summary ===\")\n",
    "print(f\"FP16 PPL:                 {fp_ppl:.4f}\")\n",
    "print(f\"INT{Target_Bits} PPL:           {q_ppl:.4f}\")\n",
    "\n",
    "print(f\"\\nFP16 ARC-Easy:            acc={fp_arc_easy_acc:.4f}  ({fp_arc_easy_correct}/{fp_arc_easy_total})\")\n",
    "print(f\"FP16 ARC-Challenge:       acc={fp_arc_ch_acc:.4f}   ({fp_arc_ch_correct}/{fp_arc_ch_total})\")\n",
    "print(f\"INT{Target_Bits} ARC-Easy:      acc={q_arc_easy_acc:.4f}  ({q_arc_easy_correct}/{q_arc_easy_total})\")\n",
    "print(f\"INT{Target_Bits} ARC-Challenge: acc={q_arc_ch_acc:.4f}   ({q_arc_ch_correct}/{q_arc_ch_total})\")\n",
    "\n",
    "print(f\"\\nFP16 SQuAD-F1:            {fp_squad_f1:.4f}  (N={fp_squad_n})\")\n",
    "print(f\"INT{Target_Bits} SQuAD-F1:      {q_squad_f1:.4f}  (N={q_squad_n})\")\n",
    "\n",
    "print(f\"\\nFP16 CNN-DM BLEU:         {fp_bleu:.4f}  (N={fp_cnn_n})\")\n",
    "print(f\"FP16 CNN-DM ROUGE-L:      {fp_rougeL:.4f}\")\n",
    "print(f\"INT{Target_Bits} CNN-DM BLEU:   {q_bleu:.4f}  (N={q_cnn_n})\")\n",
    "print(f\"INT{Target_Bits} CNN-DM ROUGE-L:{q_rougeL:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
